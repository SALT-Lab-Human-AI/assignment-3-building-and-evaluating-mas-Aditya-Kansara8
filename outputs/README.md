# Evaluation Results Directory

This directory contains evaluation results generated by running the system evaluation.

## Files Generated

When you run `python main.py --mode evaluate`, the following files are generated:

### 1. `evaluation_YYYYMMDD_HHMMSS.json`
Complete evaluation results in JSON format, including:
- Summary statistics (total queries, success rate, average scores)
- Scores by criterion (relevance, evidence_quality, factual_accuracy, safety_compliance, clarity)
- Detailed results for each query including:
  - Original query
  - Generated response
  - Evaluation scores from each judge prompt
  - Ground truth (if available)
  - Metadata (sources, citations, etc.)

### 2. `evaluation_summary_YYYYMMDD_HHMMSS.txt`
Human-readable summary of evaluation results including:
- Overall statistics
- Average scores by criterion
- Best and worst performing queries

## Running Evaluation

To generate these files, run:

```bash
python main.py --mode evaluate
```

The evaluation will:
1. Load test queries from `data/example_queries.json`
2. Process each query through the multi-agent system
3. Evaluate responses using LLM-as-a-Judge with 2 independent prompts
4. Generate aggregated statistics and detailed results
5. Save results to this directory

## Results Structure

### Summary Statistics
- `total_queries`: Number of test queries evaluated
- `successful`: Number of queries processed successfully
- `failed`: Number of queries that failed
- `success_rate`: Percentage of successful queries

### Scores
- `overall_average`: Average overall score across all queries
- `by_criterion`: Average scores for each evaluation criterion:
  - `relevance`: How relevant the response is to the query
  - `evidence_quality`: Quality of citations and evidence
  - `factual_accuracy`: Factual correctness
  - `safety_compliance`: Safety of the content
  - `clarity`: Clarity and organization

### Detailed Results
Each query result includes:
- `query`: Original query text
- `response`: Generated response from the system
- `evaluation`: Detailed evaluation scores and reasoning
- `metadata`: System metadata (sources, citations, etc.)
- `ground_truth`: Expected answer (if provided)

## Using Results in Technical Report

The evaluation results in this directory should be referenced in your technical report's "Evaluation Setup and Results" section. Include:

1. **Evaluation Setup**: Describe the test queries used, judge prompts, and criteria
2. **Results**: Present aggregate scores and key findings
3. **Error Analysis**: Analyze failed queries and low-scoring responses
4. **Interpretation**: Discuss what the results mean for system performance

## Example Analysis

To analyze results programmatically:

```python
import json
from pathlib import Path

# Load evaluation results
results_file = Path("outputs/evaluation_20240115_103045.json")
with open(results_file) as f:
    report = json.load(f)

# Access summary
print(f"Success Rate: {report['summary']['success_rate']:.2%}")
print(f"Overall Score: {report['scores']['overall_average']:.3f}")

# Analyze by criterion
for criterion, score in report['scores']['by_criterion'].items():
    print(f"{criterion}: {score:.3f}")

# Find low-scoring queries
low_scores = [
    r for r in report['detailed_results']
    if r.get('evaluation', {}).get('overall_score', 1.0) < 0.6
]
print(f"\nQueries with score < 0.6: {len(low_scores)}")
```
